tag: 
alg: ddpg
learn: hgg
subgoal_data_path: subgoal_data.npz
gamma: 0.98
eps_act: 0.3
std_act: 0.2
pi_lr: 0.001
q_lr: 0.001
act_l2: 1.0
polyak: 0.95
epochs: 20
cycles: 20
episodes: 50
timesteps: 50
train_batches: 20
her_ratio: 0.8
pool_rule: full
hgg_pool_size: 1000
clip_return: 50.0
n_test_rollouts: 1
evaluate_episodes: 10
env: FetchReach-v1
env_type: gym
goal_based: True
sparse_reward: True
reward_type: sparse
buffer_size: 100000
dynamics_buffer_size: 100000
fake_buffer_size: 10000
gen_buffer_size: 10000
dynamic_batchsize: 16
gen_batchsize: 16
warmup: 2000
coll_r: 0.1
inner_r: 0.8
outer_r: 1.0
buffer_type: energy
hgg_L: 10
hgg_c: 3.0
her: future
her_k: 4
save_acc: 0.0
save_episodes: 10
default_log_dir: ./log/temp
use_waypoints: True
waypoint_num: 5
path_reward_weight: 1.5
optimize_trajectory: True
exp: exp_1
device: 0
env_name: walker2d-medium-expert-v2
dir: results
seed: 0
num_steps_per_epoch: 1000
batch_size: 256
lr_decay: False
early_stop: False
save_best_model: False
discount: 0.99
tau: 0.005
T: 5
beta_schedule: vp
algo: ql
ms: offline
*** network initialization complete ***
